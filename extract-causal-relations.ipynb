{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import code libraries to send requests, to clean data, convert dates, etc...\n",
    "import requests\n",
    "import re\n",
    "from pprint import pprint\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function for date conversion (pushshift.io reddit database server requires posix style date)\n",
    "def date2posix(date):\n",
    "    return int(datetime.strptime(date, '%Y-%m-%d').strftime('%s'))\n",
    "\n",
    "def posix2date(posix):\n",
    "    return datetime.utcfromtimestamp(posix).strftime('%Y-%m-%d')\n",
    "\n",
    "def clean(text):\n",
    "    return re.sub('[^a-zA-Z0-9 .,@!?%&\\t\\n;:\\-())]', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Retrieve the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First comment:\n",
      "Canada is turning into a socialist hellhole.  Free speech is done. Global warming is our burden somehow.  Immigrants leaking in like the Titantic.\n",
      "In terms of priority, yes. In terms of importance, no.\n",
      "\n",
      "Let me explain. I deeply care about healtcare, war and stopping global warming... but with a Congress that’s accountable to corporations before people, we will not get the solutions we need.  \n",
      "\n",
      "Basically what I’m saying is: let’s fix Congress so that they actually do what is right. Otherwise it’s like adding water to a pool that has a huge crack in its base. Sure, it will be usable for longer, but sooner or later it’s unsustainable\n"
     ]
    }
   ],
   "source": [
    "# Prepare request to send download the reddit comments from the website database\n",
    "query_arguments = {'q':         'global warming',\n",
    "                   'subreddit': 'politics',\n",
    "                   'sort':      'asc',\n",
    "                   'size':      '3000',\n",
    "                   'after':     date2posix('2019-1-1'),\n",
    "                   'before':    date2posix('2019-4-1')}\n",
    "\n",
    "# Send request\n",
    "pol_response = requests.get('https://api.pushshift.io/reddit/search/comment', params=query_arguments)\n",
    "\n",
    "# Retrieve the 'data' field from request\n",
    "pol_comments = pol_response.json().get('data')\n",
    "\n",
    "query_arguments = {'q':         'global warming',\n",
    "                   'subreddit': 'The_Donald',\n",
    "                   'sort':      'asc',\n",
    "                   'size':      '3000',\n",
    "                   'after':     date2posix('2019-1-1'),\n",
    "                   'before':    date2posix('2019-4-1')}\n",
    "\n",
    "# Send request\n",
    "td_response = requests.get('https://api.pushshift.io/reddit/search/comment', params=query_arguments)\n",
    "\n",
    "# Retrieve the 'data' field from request\n",
    "td_comments = td_response.json().get('data')\n",
    "\n",
    "# e.g the first comment in this list of comments:\n",
    "print('First comment:')\n",
    "print(td_comments[1].get('body'))\n",
    "print(pol_comments[1].get('body'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Named Entity Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Al Gore PERSON', 22),\n",
       " ('Trump PERSON', 9),\n",
       " ('Global PERSON', 9),\n",
       " ('Maurice Strong PERSON', 6),\n",
       " ('Crichton PERSON', 5),\n",
       " ('Hillary PERSON', 5),\n",
       " ('Obama PERSON', 4),\n",
       " ('Soros PERSON', 4),\n",
       " ('Left PERSON', 4),\n",
       " ('Agenda 21 PERSON', 3)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "entities_td = []\n",
    "\n",
    "# for each comment:\n",
    "for comment in td_comments:\n",
    "    \n",
    "    # extract the named entities\n",
    "    entities = sp(comment['body'])\n",
    "    \n",
    "    # For each entity add it to the list to be counted\n",
    "    for word in entities.ents:\n",
    "        #print(word.text, word.label_)\n",
    "        \n",
    "        # Filter for person only entities \n",
    "        if word.label_ == 'PERSON':\n",
    "            entities_td.append(word.text + ' ' + word.label_)\n",
    "\n",
    "# Count the different entities in the list\n",
    "counted_entities = Counter(entities_td)\n",
    "\n",
    "# Print the 10 most common found entities\n",
    "counted_entities.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a word tree form text https://www.jasondavies.com/wordtree/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Retrieve the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First comment:\n",
      "Canada is turning into a socialist hellhole.  Free speech is done. Global warming is our burden somehow.  Immigrants leaking in like the Titantic.\n",
      "In terms of priority, yes. In terms of importance, no.\n",
      "\n",
      "Let me explain. I deeply care about healtcare, war and stopping global warming... but with a Congress that’s accountable to corporations before people, we will not get the solutions we need.  \n",
      "\n",
      "Basically what I’m saying is: let’s fix Congress so that they actually do what is right. Otherwise it’s like adding water to a pool that has a huge crack in its base. Sure, it will be usable for longer, but sooner or later it’s unsustainable\n"
     ]
    }
   ],
   "source": [
    "# Prepare request to send download the reddit comments from the website database\n",
    "query_arguments = {'q':         'global warming',\n",
    "                   'subreddit': 'politics',\n",
    "                   'sort':      'asc',\n",
    "                   'size':      '100',\n",
    "                   'after':     date2posix('2019-1-1'),\n",
    "                   'before':    date2posix('2019-4-1')}\n",
    "\n",
    "# Send request\n",
    "pol_response = requests.get('https://api.pushshift.io/reddit/search/comment', params=query_arguments)\n",
    "\n",
    "# Retrieve the 'data' field from request\n",
    "pol_comments = pol_response.json().get('data')\n",
    "\n",
    "query_arguments = {'q':         'global warming',\n",
    "                   'subreddit': 'The_Donald',\n",
    "                   'sort':      'asc',\n",
    "                   'size':      '100',\n",
    "                   'after':     date2posix('2019-1-1'),\n",
    "                   'before':    date2posix('2019-4-1')}\n",
    "\n",
    "# Send request\n",
    "td_response = requests.get('https://api.pushshift.io/reddit/search/comment', params=query_arguments)\n",
    "\n",
    "# Retrieve the 'data' field from request\n",
    "td_comments = td_response.json().get('data')\n",
    "\n",
    "# e.g the first comment in this list of comments:\n",
    "print('First comment:')\n",
    "print(td_comments[1].get('body'))\n",
    "print(pol_comments[1].get('body'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) extract causal relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare request to send this comment(s) to the causal relation extractor\n",
    "\n",
    "# prepare a list of texts to send (but clean it first)\n",
    "texts_td = [clean(comment['body']) for comment in td_comments]\n",
    "texts_pol = [clean(comment['body']) for comment in pol_comments]\n",
    "\n",
    "# Send the request\n",
    "json_data = {'texts': texts_td, 'frames': ['Causation']}\n",
    "response  = requests.post('https://penelope.vub.be/semantic-frame-extractor/texts-extract-causes-effects', json=json_data)\n",
    "data_td      = response.json()['causalRelations']\n",
    "\n",
    "# Send the request\n",
    "json_data = {'texts': texts_pol, 'frames': ['Causation']}\n",
    "response  = requests.post('https://penelope.vub.be/semantic-frame-extractor/texts-extract-causes-effects', json=json_data)\n",
    "data_pol      = response.json()['causalRelations']\n",
    "\n",
    "# Create a list of causes and a list of effects\n",
    "causes_td = []\n",
    "effects_td = []\n",
    "for frame in data_td:\n",
    "    causes_td.append(frame['cause'])\n",
    "    effects_td.append(frame['effect'])\n",
    "    \n",
    "# Create a list of causes and a list of effects\n",
    "causes_pol = []\n",
    "effects_pol = []\n",
    "for frame in data_pol:\n",
    "    causes_pol.append(frame['cause'])\n",
    "    effectspol.append(frame['effect'])\n",
    "    \n",
    "\n",
    "# get the most occuring causes\n",
    "# Instead of just counting sentences, you could cluster the different according to their similarity\n",
    "counted_causes_td = Counter(causes_td).most_common(20)\n",
    "counted_effects_td = Counter(effects_td).most_common(20)\n",
    "print(counted_causes_td)\n",
    "\n",
    "counted_causes_pol = Counter(causes_pol).most_common(20)\n",
    "counted_effects_pol = Counter(effects_pol).most_common(20)\n",
    "print(counted_causes_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Plot the most frequent effects or causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# create a barchart of the most frequent effects of subreddit the_donald\n",
    "\n",
    "keys = counted_effects_td.keys()\n",
    "y_pos = np.arange(len(keys))\n",
    "counts = [counted_effects_td[k] for k in keys]\n",
    "error = np.random.rand(len(keys))\n",
    "\n",
    "plt.barh(y_pos, counts, alpha=0.4)\n",
    "plt.yticks(y_pos, keys)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a barchart of the most frequent effects of subreddit politics\n",
    "\n",
    "keys = counted_effects_pol.keys()\n",
    "y_pos = np.arange(len(keys))\n",
    "counts = [counted_effects_pol[k] for k in keys]\n",
    "error = np.random.rand(len(keys))\n",
    "\n",
    "plt.barh(y_pos, counts, alpha=0.4)\n",
    "plt.yticks(y_pos, keys)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
